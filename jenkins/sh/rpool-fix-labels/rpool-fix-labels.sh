#!/bin/bash

#
# This file and its contents are supplied under the terms of the
# Common Development and Distribution License ("CDDL"), version 1.0.
# You may only use this file in accordance with the terms of version
# 1.0 of the CDDL.
#
# A full copy of the text of the CDDL should have accompanied this
# source.  A copy of the CDDL is also available via the Internet at
# http://www.illumos.org/license/CDDL.
#

#
# Copyright (c) 2017 by Delphix. All rights reserved.
#

source ${JENKINS_DIRECTORY}/sh/library/common.sh

#
# This script is a giant hack, but it's a necessary one.
#
# When creating the Amazon EC2 AMI, the process starts by performing an
# installation from an ISO install media, and the "rpool" is generated
# during this installation. The VM configuration that is used when
# performing the installation is such that the only disk available at
# install time is the "c4t0d0" device, so this is the path used to
# create the "rpool".
#
# This becomes a problem when the VM image generated by the ISO install
# is converted to an Amazon AMI, and an EC2 instance is generated from
# that AMI. When inside the EC2 instance, the device used by the "rpool"
# will now be "c2t0d0" instead of "c4t0d0", as it was during the ISO
# installation. When this occurs, the ZFS VDEV labels will not be
# updated to reflect the new location of the root pools disk. Thus, when
# executing "zpool list -v", it'll still report "c4t0d0" instead of the
# correct location of "c2t0d0".
#
# This isn't a big deal in and of itself, but it then results in
# failures when "beadm activate" is run via "onu" later in the Jenkins
# automation. The failure is due to "beadm activate" using
# "zpool_vdev_name" to get the root pool's device name, which returns
# "c4t0d0", and then this value being passed directly to "installboot".
# The "installboot" command will attempt to install using the
# "/dev/rdsk/c4t0d0" device, which doesn't exist, causing the "onu" to
# fail.
#
# Thus, we use this script to cause the ZFS VDEV labels for "rpool" to
# be rewritten such that they contain the correct device information.
# Normally this occurs during the "zpool import" process, but a root
# pool is imported much differently than a "regular" pool, and the VDEV
# labels aren't updated for a root pool "import". We workaround this
# situation by using a clever combination of "zpool attach" and
# "zpool detach" to cause the VDEVs labels for the root pool to get
# updated and reflect the correct device paths.
#
# Additionally, the expected device names are hardcoded here; so this is
# subject to break if the device names were to changed (e.g. due to
# changes to the device enumeration code, and/or changes at the EC2
# hypervisor layer).
#
BROKEN_BOOT_DEVICE="c4t0d0"
REPAIR_BOOT_DEVICE="c2t0d0"
EXTRA_DEVICE="c2t1d0"

function boot_devices() {
	zpool list -vH rpool \
		| awk '! /rpool|mirror|replacing|spare/ { print $1 }'
}

function all_devices() {
	#
	# Due to deficiencies in the illumos drivers, it's possible (and
	# expected when running on EC2) for an IDE device and XDF device
	# to be visible from within the VM, even though there's only a
	# single "physical" disk attached to the VM; i.e. both the IDE
	# and XDF devices map to the same physical disk.
	#
	# For us, we're only interested in using the XDF device, so we
	# filter out the IDE devices here, as a workaround.
	#
	sudo diskinfo -H | awk '! /ATA/ { print $2 }' | sort | paste -sd ' '
}

function wait_for_resilver_to_complete() {
	while ! log_must zpool status \
		| tee /dev/stderr \
		| log_must grep scan \
		| log grep -q resilvered; do
		sleep 60
	done
}

#
# This simply emits the "boot device" and "all devices" as output, which
# is intended to get capture and logged via the Jenkins job. Having this
# information readily available can help aid debugging efforts if this
# script happens to fail.
#
# Additionally, if the commands don't produce any output at all, this
# will pro-actively fail the script as that's not expected.
#
log_must boot_devices | log_must tee /dev/stderr | log_must grep -q .
log_must all_devices | log_must tee /dev/stderr | log_must grep -q .

#
# Assert "rpool" starts in the expected configuration, and there's no
# unexpected devices found on the system.
#
log_must test "$(boot_devices)" == "$BROKEN_BOOT_DEVICE"
log_must test "$(all_devices)" == "$REPAIR_BOOT_DEVICE $EXTRA_DEVICE"

#
# Now that we've verified the system is in the state we expect, we can
# proceed to repair the "rpool" label by performing a clever dance.
#

log_must sudo zpool attach rpool "$BROKEN_BOOT_DEVICE" "$EXTRA_DEVICE"
log_must wait_for_resilver_to_complete

log_must sudo zpool detach rpool "$BROKEN_BOOT_DEVICE"

log_must sudo zpool attach rpool "$EXTRA_DEVICE" "$REPAIR_BOOT_DEVICE"
log_must wait_for_resilver_to_complete

log_must sudo zpool detach rpool "$EXTRA_DEVICE"

#
# And finally, verify the dance did what we expected it to do; when
# listing the boot device it should report REPAIR_BOOT_DEVICE, when it
# previously reported BROKEN_BOOT_DEVICE.
#
log_must test "$(boot_devices)" == "$REPAIR_BOOT_DEVICE"
log_must test "$(all_devices)" == "$REPAIR_BOOT_DEVICE $EXTRA_DEVICE"
